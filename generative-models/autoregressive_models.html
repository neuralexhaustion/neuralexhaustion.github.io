<!DOCTYPE html>
<html lang="en">

<head>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
    <link href='https://fonts.googleapis.com/css?family=Source Sans Pro' rel='stylesheet'>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">

    <title>Autoregressive Models</title>
</head>

<body>
    <!-- Navigation Bar -->
    <div>
        <div class="navbar">
            <ul>
                <li class='navr'><a class='nava' href="#">Contact</a></li>
                <li class='navr'><a class='nava' href="#">Blog</a></li>
                <li class='navl'><a class='nava' href="../index.html">Home</a></li>
            </ul>
        </div>
        <div class="blog">
            <hr>
            <hr>
            <h1 style="color:white;margin-bottom: 0px"> <span> üçÅ Autoregressive Models</span> <span
                    style="float:right; color:#67AB9F;"><span style="color: rgb(246, 51, 102)">date</span>
                    13.09.2022</span></h1>

            <h2 style="padding-left: 70px; margin-top: 10px;"><span> Motivation, Key Ideas</span> <span
                    style="float:right;color: #67AB9F; font-family: 'Source Sans Pro Semi-Bold'; font-size:2.75rem; font-weight: 700;"><span
                        style="color: rgb(246, 51, 102);">by</span style> Hidir Yesiltepe</span></h2>
            <br>
            <hr>
            <hr>
            <br>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/cover.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="margin-top: 10px; font-size:14px; color:#757575;"> A Digital Fantasy Image
                        Generated by Dalle-2</figcaption>
            </figure>
            <h1>üçÅ Introduction </h1>
            <p style="margin-bottom: 1rem;">
                Autoregressive models are a sequential deep generative models which are among the constituent part of
                the
                state-of-the art models
                in different domains such as <span style="color:rgb(246, 51, 102)">Image Generation</span>,
                <span style="color:rgb(246, 51, 102)">Audio Generation</span> ,
                <span style="color:rgb(246, 51, 102)">Gesture Generation</span> and
                <span style="color:rgb(246, 51, 102)">Large Language Models.</span>

            </p>
            <p> Since autoregressive models are sequential, they are similar to <span
                    style="color:rgb(246, 51, 102)">RNNs</span> but they differ in the method.
                While RNNs are recurrent, autoregressive models are feed-forward yet both methods are supervised.
            </p>
            <br>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/network2.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;"> Figure 1:
                        Both RNNs and
                        Autoregressive Models process data sequentially. (Left) RNN output at time-step T depends not
                        only the current input, but also the previous inputs and the dependency is provided through
                        hiddent states. (Right) The output of an autoregressive model also depends on current and
                        previous inputs but in this case previous outputs are explicitly given to the model as inputs.
                    </figcaption>
            </figure>
            <br>
            <br>

            <p>
                On the other hand, because autoregressive models are a type of generative models, their expressiveness
                capability is often compared against <span style="color:rgb(246, 51, 102)">VAEs</span> and <span
                    style="color:rgb(246, 51, 102)">GANs</span> but there are some core differences. First of all,
                although autoregressive models and VAEs are explicit models, due to intractable likelihood computation,
                VAEs optimize <span style="color:rgb(246, 51, 102)">Evidence Lower Bound</span> whereas in
                autoregressive models likelihood is optimized directly.
                When it comes to comparing autoregressive models with GANs, GANs are implicit models which optimizes
                minimax objective as opposed to explicit likelihood based autoregressive models.
            </p>
            <br>

            <div data-stale="false" width="1160" class="element-container css-4aataf e1tzin5v3">
                <div class="css-1a32fsj e19lei0e0">
                    <div data-testid="stTable" class="css-1ec096l edw49t14">
                        <table class="css-zuelfj edw49t13">
                            <thead>
                                <tr>
                                    <th class="blank css-c34i5s edw49t11">&nbsp;</th>
                                    <th scope="col" class="col_heading level0 col0 css-c34i5s edw49t11"
                                        style="text-align: left;">Model</th>
                                    <th scope="col" class="col_heading level0 col1 css-c34i5s edw49t11"
                                        style="text-align: left;">Main Objective</th>
                                    <th scope="col" class="col_heading level0 col2 css-c34i5s edw49t11"
                                        style="text-align: left;">Explicit/Implicit</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <th scope="row" class="row_heading level0 row0 css-c34i5s edw49t11">0</th>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Autoregressive Models</td>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Log-Likelihood</td>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Explicit</td>
                                </tr>
                                <tr>
                                    <th scope="row" class="row_heading level0 row1 css-c34i5s edw49t11">1</th>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Variational Autoencoders
                                        (VAEs)</td>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Evidence Lower Bound
                                        (ELBO)</td>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Explicit</td>
                                </tr>
                                <tr>
                                    <th scope="row" class="row_heading level0 row2 css-c34i5s edw49t11">2</th>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Generative Adversarial
                                        Networks (GANs)</td>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Minimax</td>
                                    <td class="css-4sszyo edw49t12" style="text-align: left;">Implicit</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
            <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                Table 1: General Properties of Deep Generative Models
            </figcaption>
            <br>
            <h1>üçÅ Chain Rule</h1>
            <p>We start our discussion on autoregressive models with the chain rule of probability. From now on, we
                assume that we have an access to a dataset of N dimensional <span
                    style="color:rgb(246, 51, 102)">binary</span> datapoints and the cardinality of M:
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ \mathcal{D} = \{x_i\}_{i=1}^{\mathcal{M}}
                \hspace{60px} x_i \in R^{\mathcal{N}} \tag{1}$$</p>
            <p>The chain rule of probability states that joint probability distribution can be factorized into
                conditional probability distributions. This fact constitutes the backbone of <span
                    style="color:rgb(246, 51, 102)">Bayesian Networks</span> and
                <span style="color:rgb(246, 51, 102)">Autoregressive Models. </span>
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_i) = P(x_1, ..., x_N) =
                P(x_1)\prod_{j=1}^{\mathcal{N}} P(x_j | x_1, ..., x_{j-1}) \tag{2} $$ </p>
            <p>which means the probability of any random variable in the set is conditionally dependent on all the
                random variables preceding it and further, the dependence order is <span
                    style="color:rgb(246, 51, 102)">predefined</span> and <span
                    style="color:rgb(246, 51, 102)">fixed.</span> Such a
                dependence relationship can be graphically described as:</p>

            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/graphical_chain.png" width="600" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 2: Chain Rule Graphical Representation as Bayes Network
                    </figcaption>
            </figure>
            <br>
            <p>Chain rule of probability is a useful factorization because often dealing with conditional probabilities
                is more convenient than dealing with joint probability at once. Now the question is <span
                    style="color:rgb(246, 51, 102)">how can we represent
                    such a dependence relationship?</span></p>

            <h1>üçÅ Representation</h1>
            <p>We can represent the <span style="color:rgb(246, 51, 102)">Equation [2]</span> in several modelling
                approaches with a trade-off between expressiveness
                and the complexity of the models. Further in this section, we will experience that complexity of the
                models creates an immense limitation such that at the end we will have to make some <span
                    style="color:rgb(246, 51, 102)">modelling assumptions.</span>
            </p>

            <h2>üçÅ Tabular Representation</h2>
            <p> We can represent the conditional probabilities with a tabular form which is called <span
                    style="color:rgb(246, 51, 102)">Conditional Probability
                    Table (CPT).</span> Representing conditional probabilities in a tabular form means that for each
                conditional
                probability there is a CPT such that every configuration of random variables involving in the
                conditional
                probability have a corresponding likelihood. For the sake of visualizing, suppose we have 3 <span
                    style="color:rgb(246, 51, 102)">binary</span> random
                variables.</p>

            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/graphical_chain2.png" width="500" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 3: Graphical Representation of Dependencies Between Binary Random Variables A, B and C
                    </figcaption>
            </figure>
            <br>
            <p>The joint probability of random variables can be factorized into conditional probabilities by chain rule
                as follows:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';"> $$ P(A, B, C) = P(A)P(B|A)P(C|A, B) \tag{3}$$ </p>
            <p>As a result of this factorization, corresponding CPTs can be viewed as following tables:</p>

            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/table.png" width="100%" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Table 2: Conditional Probability Tables. (Left) Random variable A does not depend on any other
                        random variable in the set. (Middle) Random variable B depends on random variable A. (Right)
                        Random variable C depends both random variables A and B.
                    </figcaption>
            </figure>
            <br>

            <p>Above, you see the CPT tables representing each conditional probability along with <span
                    style="color:rgb(246, 51, 102)">minimum number of
                    parameters</span> required to represent each table. For example, since for random variable A we
                have:</p>

            <p style="font-size: 1.8rem; font-family: 'KaTeX';"> $$ P(A = 0) = 1 - P(A=1) \tag{4} $$</p>
            <p>we don't have to use two variables here, only one variable can represent the entire table. Same applies
                for others by grouping the <span style="color:rgb(246, 51, 102)">conditioning variables.</span> Now,
                let's see the advantages and disadvantages of
                tabular representation.</p> <br>
            <p>The advantage of tabular modelling choice is that we can represent any joint distribution exactly which
                means that among the other modelling choices, tabular form representation have the highest
                expressiveness. But it turns out that to be able to represent joint distribution in a tabular form we
                need <span style="color:rgb(246, 51, 102)">exponential size </span>parameters!</p>

            <p style="font-size: 1.6rem; font-family: 'KaTeX';"> $$ P(x_1) : 2^0 \text{ parameters} $$</p>
            <p style="font-size: 1.6rem; font-family: 'KaTeX';"> $$ P(x_2|x_1) : 2^1 \text{ parameters} $$</p>
            <p style="font-size: 1.6rem; font-family: 'KaTeX';"> $$ P(x_3|x_1, x_2) : 2^2 \text{ parameters} $$</p>
            <p style="font-size: 1.6rem; font-family: 'KaTeX';"> $$ ... $$</p>
            <p style="font-size: 1.6rem; font-family: 'KaTeX';"> $$ P(x_N|x_1, ..., x_{N-1}) : 2^{N-1} \text{
                parameters} $$</p>
            <p>By geometric sum formula, we have:</p>
            <p style="font-size: 1.6rem; font-family: 'KaTeX';"> $$ 2^0 + 2^1 + 2^2 + \text{ ... } + 2^{N-1} = \frac{1 -
                2^N}{1 - 2} = 2^N - 1 \text{ total parameters} \tag{5} $$</p>

            <p>To better understand what exactly exponential size parameters mean, let's give a concrete example.
                <span style="color:rgb(246, 51, 102)">Binarized MNIST</span> dataset consists of 28 x 28 images which
                means each data point (image) in the dataset
                lies in 784 dimensional space.
            </p>

            <p style="font-size: 1.8rem; font-family: 'KaTeX';"> $$ x_i \in \{0, 1\}^{784} $$</p>
            <p>To represent any possible joint probability in a tabular form as we described above, we would need:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';"> $$ 2^{784} \approx 10^{236} $$</p>
            <p>probabilities to estimate which is obviously not tractable. Observe that, current representation does not
                have any independence assumption, each random variable is conditionally dependent <span
                    style="color:rgb(246, 51, 102)">all the preceding
                    random variables.</span> An improvement on the parameter size would be introducing <span
                    style="color:rgb(246, 51, 102)">conditional independence
                    assumptions</span> in which our next representation exactly does.</p>

            <h2>üçÅ Bayes Network</h2>
            <p> The previous approach to represent joint probability with individiual conditional probabilities as a
                result
                of chain rule was <span style="color:rgb(246, 51, 102)">fully general</span> and there was no <span
                    style="color:rgb(246, 51, 102)">conditional assumptions.</span> Now, let's consider the same 3
                binary random variable setting again and introduce a conditional independence between B and C.</p>


            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/graphical_chain3.png" width="500" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 4: Graphical Representation of Dependencies Between Binary Random Variables A, B and C
                        with Independence Between B and C
                    </figcaption>
            </figure>
            <br>

            <p>Observe that above configuration is almost same as before except that now random variables B and C are
                independent. Depending upon the problem at hand, this might or might not be a huge assumption.
                Nevertheless, we make it in order to reduce the number of parameters of the model. Note that, <span
                    style="color:rgb(246, 51, 102)">there is
                    no particular reason</span> behind we chose random variables B and C, we could have chosen some
                other
                combination to make it independent as well.</p>

            <br>
            <p>With that being said, we can now represent the joint probability in terms of conditional probabilities
                as:</p>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/prev_current.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">

            </figure>
            <p>We can again utilize from <span style="color:rgb(246, 51, 102)">CPT</span> represent conditional
                probabilities as follows:</p>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/table2.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Table 3: Conditional Probability Tables. (Left) Random variable A does not depend on any other
                        random variable in the set. (Middle) Random variable B depends on random variable A. (Right)
                        Random variable C depends only on random variable A.
                    </figcaption>
            </figure>
            <br>
            <p>One might adopt a modelling design such that each random variable depends only on the random variable
                preceding it. If we generalize this assumption over N random variables we get:</p>

            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/graphical_chain4.png" width="600" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 5: Graphical Representation of Dependencies over N Random Variable
                    </figcaption>
            </figure>
            <br>
            <p>In this dependence relationship, the joint probability can be written as:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_1, ..., x_N) = P(x_1)\prod_{j=2}^N P(x_j|x_{j-1})
                \tag{6}$$</p>
            <p>Now let's calculate the number of parameters required to represent joint distribution in a tabular form
                with the assumption we have made:</p>

            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_1) : 2^0 \text{ parameters}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_2|x_1) : 2^1 \text{ parameters}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_3|x_2) : 2^1 \text{ parameters}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ ... $$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_N|x_{N-1}) : 2^1 \text{ parameters}$$</p>
            <br>
            <p>As a result, in total we have:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ 1 + (N - 1) \times 2 = 2N - 1 \text{ total
                parameters}$$
            </p>
            <p>It turns out that, we didn't just reduce the number of parameters drastically but also the <span
                    style="color:rgb(246, 51, 102)">representation
                    capacity.</span> Obviously a model with exponential size parameters and a model with linear size
                parameters
                does not have the same capacity. <span style="color:rgb(246, 51, 102)">Can't we just create an
                    expressive but yet efficient model that makes no conditional assumption?</span></p>

            <h2>üçÅ Autoregressive Models</h2>
            <p>The main idea behind Autoregressive Models is representing the conditional distributions with Neural
                Networks without making any conditional independence assumption. All we need to do is parameterizing the
                conditional distributions with some weight vector Œ∏.</p>

            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/graphical_chain5.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 6: Graphical Representation of Parameterized Dependencies over N Random Variable
                    </figcaption>
            </figure>
            <br>
            <p>An important note here, as you can see in the figure above there is <span
                    style="color:rgb(246, 51, 102)">no conditional assumption</span> in autoregressive models, all the
                random variables depends each and every random variable preceding it and furhter, dependence is provided
                by parameters Œ∏. We can represent the joint distribution as:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$P(x_1, ..., x_N) = P(x_1)\prod_{j=2}^N P_{Neural}(x_j|
                x_1, ..., x_{j-1}; Œ∏) \tag{7}$$</p>
            <br>
            <p>In the above representation of chain rule, probabilities with <span
                    style="color:rgb(246, 51, 102)">Neural</span> subscript are special functional form for
                conditionals. On the other hand, the very first random variable is drawn from a known distribution. It
                is best understood by giving a concrete example. Suppose we have an access to a <span
                    style="color:rgb(246, 51, 102)">Binarized Icon Dataset</span> in which each element of the dataset
                is of size 7 x 7.
            </p>
            <br>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/(7x7).png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 7:
                    </figcaption>
            </figure>

            <p>More explicitly, we can write the equations describing above autoregressive image generative models as
                following:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P(x_1 = 1; Œ∏_1) = Œ∏_1 \hspace{80px} P(x_1 = 0; Œ∏_1) =
                1 - Œ∏_1 \tag{8}$$</p>
            <p> The very first random variable does not depend any other random variable so it is drawn from <span
                    style="color:rgb(246, 51, 102)"> Bernoulli
                    Distribution</span> with a success parameter Œ∏<sub>1</sub>. A significant remainder here is that
                P(x<sub>1</sub> ; Œ∏<sub>1</sub>) is <span style="color:rgb(246, 51, 102)">not a neural network.</span>
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P_{Neural}(x_2 = 1 | x_1; \theta_2) =
                \sigma(\theta^2_0 + \theta^2_1 \times x_1) \tag{9}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P_{Neural}(x_3 = 1 | x_1, x_2; \theta_3) =
                \sigma(\theta^3_0 + \theta^3_1 \times x_1 + \theta^3_2 \times x_2) \tag{10}$$</p>

            <p>Where sigma is a non-linear function. Often, the result of the P<sub>Neural</sub> is considered as a
                <span style="color:rgb(246, 51, 102)">logit</span> and it is represented as:
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P_{logit} = P_{Neural}$$</p>
            <p>Let's attribute for what a logit is and why this result is associated with the term logit. The logit
                <b>L</b> of a probability <b>P</b> is defined as:
            </p>

            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ L = ln\frac{P}{1-P} \tag{11}$$</p>
            <p>which is the natural logarithm of odds. There are two important properties of logit. Firstly, the range
                of the logit is (-&infin;, &infin;). Secondly, the inverse of the logit function is the sigmoid
                function:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ P = \frac{1}{1 + e^{-L}} = Sigmoid(L) \tag{12}$$</p>
            <p>As a summary, logits are unnormalised predictions which are then normalised via softmax. Now, we know the
                main motivation behind autoregressive models and how a simple autoregressive model is structured. Next,
                we are going to take a look at the <span style="color:rgb(246, 51, 102)">objective</span> of
                autoregressive models.
            </p>


            <h1>üçÅ Main Objective</h1>
            <p>Autoregressive models are explicit models which further means that they optimize the underlying joint
                distribution
                directly via <span style="color:rgb(246, 51, 102)">Maximum Likelihood Estimation (MLE).</span> Most
                importantly, maximizing the MLE objective is not an arbitrary choice but is a natural consequence of KL
                divergence between <b>P<sub>data</sub></b> and <b>P<sub>model</sub></b>. In this section we will take a
                look at the natural relation between the KL Divergence and Maximum Likelihood Estimation.</p>

            <h2>üçÅ KL & MLE Relationship</h2>
            <p>The ultimate goal of an autoregressive model is capturing the underlying probability distribution of the
                data. To quantify how close two distributions are we attempt to <span
                    style="color:rgb(246, 51, 102)">minimize the KL Divergence</span> between the
                data distribution
                <b>P<sub>data</sub></b> and distribution captured by an autoregressive model <b>P<sub>model</sub></b>:
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ \underset{\theta}{\text{minimize}} \hspace{30px}
                KL(P_{data}(x) || P_{model}(x ;\theta)) \tag{13}$$</p>
            <p>If we write the open form of KL to eliminate the terms which do not depend on parameter theta, we obtain:
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ \underset{\theta}{\text{minimize}} \hspace{30px}
                \sum_x P_{data}(x) log \frac{P_{data}(x)}{P_{model}(x;\theta)} \tag{14}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \underset{\theta}{\text{minimize}} \hspace{30px}
                \sum_x P_{data}(x) log P_{data}(x) - \sum_x P_{data}(x) log P_{model}(x;\theta) \tag{15}$$</p>
            <p>Where the first term is called the <span style="color:rgb(246, 51, 102)">negative entropy</span> of the
                random variable X and does not depend on the parameter theta. If we eliminate this term and write the
                second term as an expectation we obtain:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \underset{\theta}{\text{minimize}} \hspace{30px}
                -\mathbb{E}_{x \sim P_{data}}[log P_{model}(x;\theta)] \tag{16}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \underset{\theta}{\text{maximize}} \hspace{30px}
                \mathbb{E}_{x \sim P_{data}}[log P_{model}(x;\theta)] \tag{17}$$</p>
            <p>As the last step, applying the <span style="color:rgb(246, 51, 102)">Law of Large Numbers</span> to get
                rid of expectation term yields:</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \underset{\theta}{\text{maximize}} \hspace{30px}
                \underset{n \rightarrow \infty}{\text{lim}} \sum_{i=1}^n logP_{model}(x^{(i)};\theta) \tag{18}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \underset{\theta}{\text{maximize}} \hspace{30px}
                logP_{model}(x;\theta) \tag{19}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \underset{\theta}{\text{maximize}} \hspace{30px}
                P_{model}(x;\theta) \tag{20}$$</p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ = \theta_{MLE} \tag{21}$$</p>
            <p>As the summary, since autoregressive models are likelihood based, as a consequence of minimizing the KL
                Divergence between the true underlying distribution and the distribution represented by the model we
                estimate the model parameters. It turns out that estimating the model parameters in this fashion
                equivalent to maximum likelihood estimation. In practice we minimize the negative log likelihood.</p>
            <h1>üçÅ Inference and Sampling</h1>
            <p>A consequence
                of the parameterization of the an autoregressive model is that the weight matrix is a lower triangular
                matrix.</p>
            <br>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/weight_matrix.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 8: Representation of Weight Matrix of the described Autoregressive Model is a Lower
                        Triangular Matrix.
                    </figcaption>
            </figure>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ OR $$</p>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/weight_matrix2.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 9: Representation of Weight Matrix of the described Autoregressive Model is a Lower
                        Triangular Matrix.
                    </figcaption>
            </figure>
            <p>
                <b><span style="color:rgb(246, 51, 102)">An important observation here:</span></b> Inference becomes a
                parallel computation and fast in autoregressive models. Consider the density estimation of an arbitrary
                point:
            </p>
            <p style="font-size: 1.8rem; font-family: 'KaTeX';">$$ x = [x_1, ..., x_n]^T $$</p>
            <p>All we need to do is compute the above matrix multiplication to yield log-conditionals. Then how do we
                obtain log-conditionals if our architecture models logits rather than log-probabilities directly? It is
                quite simple as well. Compute the above matrix multiplication to yield logits, then apply softmax to
                yield each probability and finally apply logarithm to each probability, it is done!</p>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/logit_to_log_prob.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 10: Transition from Logit to Probability to Log Probability
                    </figcaption>
            </figure>
            <p>Above you see the transition in which L stands for the logit. Obtaining the objective after this step is
                not a big deal. It reduces to adding each conditional log probability.</p>
            <figure>
                <div style="display: flex; justify-content:center">
                    <img src="assets/joint_log_prob.png" alt="">
                </div>
                <div style="display: flex; justify-content:center">
                    <figcaption style="text-align: center; margin-top: 10px; font-size:14px; color:#757575;">
                        Figure 11: The Objective - Log-Likelihood - Computation
                    </figcaption>
            </figure>
            <p>Sampling on the other hand is a sequential process and slow. Why is that? Since x<sub>2</sub> depends on
                x<sub>1</sub> we need to sample x<sub>1</sub> prior to x<sub>2</sub>. Further, since x<sub>3</sub>
                depends both x<sub>1</sub> and x<sub>2</sub> we need to sample those prior to x<sub>3</sub>. Following
                this fashion brings us to a sequential process.</p>
            <br>
            <hr>
            <br>
            <p style="color: #67AB9F">Our introductory discussion on Autoregressive Models ends here, <span
                    style="color: rgb(246, 51, 102)">hope you enjoyed!</span> In the upcoming posts we are going to
                explore the prominent <span style="color: rgb(246, 51, 102)">applications of Autoregressive
                    Models.</span></p>

            <br>
            <a style="text-decoration: none;" href="https://github.com/neuralexhaustion/neuralexhaustion.github.io/discussions/1">
                <p style="font-size: 3rem; font-weight: 700; color: #2196f3">üîó Give Feedback & Ask Question!</p>
            </a>
            <br><br>
            
</body>

</html>
